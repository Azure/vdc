Extending the Azure VDC Automation Toolkit
==========================================

The automation toolkit allows you to include additional resource deployments
based on your organization needs.

Creating new deployments
------------------------

The resource deployment templates used in the VDC Automation Toolkit are
standard Azure Resource Manager templates. For details on creating these
templates, consult the guide for creating [Azure Resource Manager
templates](https://docs.microsoft.com/azure/azure-resource-manager/resource-group-authoring-templates).

VDC automation allows two features that are not normally available in Resource
Manager templates:

-   The ability to reference values generated by the output of previous
    deployments.

-   The ability to pull values from the central deployment parameters file for a
    shared services or workspace and use those values in a number of individual resource
    deployment tasks.

### Create a custom deployment

When creating a new shared services or workload deployment, it's recommended that you make a uniquely named copy of one of the archetype folders containing the configuration file for the  deployment type you're creating. For instance, If you want to create a custom workload deployment, copy the *archetypes/iaas* to *archetypes/custom-workload-name*.

You can then modify this main parameters file by adding or removing new modules and updating parameter settings uniquely for your deployment. Once your modifications are complete, you can simply update the path argument when running the main vdc.py script to point to the new main parameters file location to run your new deployment.


Integrating new resource deployments into the automation toolkit
----------------------------------------------------------------

New resource deployments need to be integrated into the automation folder
structure and registered in the global shared services or workload parameters file before they
can be deployed using the automation scripts.

### Folder structure

There is a "modules" folder (containing Resource Manager deployment
templates, the corresponding parameters files, and optionally, ARM policies).

The folder name for each module deployment is used as its ID throughout the
rest of the VDC automation system. Files themselves always have the same name:
azureDeploy.json for deployment templates, and azureDeploy.parameters.json for
parameters.

As an example, if you were creating a deployment to create a Hadoop
server in your custom workload deployment, you would use the following file paths
to store your resource files:

>   *modules/hadoop/v1/azureDeploy.json*

>   *modules/hadoop/v1/azureDeploy.parameters.json*

### Using the output of dependent resource deployments

If deployment module templates have outputs defined (using standard ARM template format), the automation scripts save these
to a shared storage location. Other resource deployments can integrate these
outputs if they've defined a dependency.

These outputs need to be defined as parameters in the dependent resource's
Resource Manager deployment template. For instance, the "ops" deployment
generates the following output values:

-   oms-id

-   oms-workspace-resourceGroup

-   oms-workspace-name

Any new resource template that uses these values needs to have a dependency
defined in the main deployment parameters file and include any of these items it
uses in the parameter definitions of its deployment module's template.

When a deployment module gets processed by the automation scripts, the output
variables from previous deployments are pulled from the shared storage
location and integrated with the parameters file for the new deployment. In turn
this new deployment creates its own output in shared storage that can be used by
later deployments.

### Referencing global parameters

Within the parameters file for each resource deployment module, VDC automation allows
you to make use of values defined in the global shared services or workload deployment
parameters file. This is done by referencing the global value using the
following format: **${parameter JSON variable}**

As an example, in a deployment module's parameters file you can pull the organization name
and shared services deployment name from a shared services parameters file into a "deployment-prefix"
parameter value like this:

```javascript
   "deployment-prefix": {
       "value": "${general.organizationName}-${shared-services.deployment-name}"
   },
```

### Defining modules and dependencies

Modules need to be defined in the main parameters file for your shared services or workload before they can be used in a deployment. This is done in the module-dependencies object. This object is defined in detail as part of the [parameters documentation](03-parameters-files.md#common-module-dependency-parameters), but, in summary, consists of an optional import-module value, and an array of module definitions.

#### import-module

The import-module sets the default location for all modules in the deployment. If not specified, the scripts will default to using the vdc toolkit root. Custom paths can also be set in the module definition for individual module files. Path definitions support absolute or relative paths using the file() function.

#### Module definition

When adding a new module, you must at a minimum supply a module name and version as part of the definition. Dependencies between deployment modules are also set as part of the module definition.

See the [parameters documentation](03-parameters-files.md#common-module-dependency-parameters) for full details of the module definition object.

As an example, we can create a definition for our Hadoop example module. It is currently version 1.0 and needs to pull in values from the workload's "net" deployment module output. To add this definition, you will add the following definition to the archetypes/custom-workload-name/archetype.json file's module-dependencies object:

```javascript
    {
        "module": "hadoop",
        "source": {
            "version": "v1"
        },
        "dependencies": [
            "net",
        ]
    }
```
This definition tells the automation scripts to pull in the output values from the "net" deployment and integrate them with the "hadoop" deployment module's parameters file. The "net" output values you want to reference in the "hadoop" module need to be defined in the "hadoop" deployment template as parameters.

### Adding the deployment into the deployment resource array

To use your new deployment module with the automation toolkit, you need to add
it to the "resource-deployment-order" array stored in the same global shared services or
workload deployment parameters file. Once added, the scripts recognize the resource
name as a valid option when running deployments, and you can execute the
deployment using the main Python script discussed elsewhere in this guide.

The workload module-deployment-order array with a "hadoop-cloudbreak" deployment added looks
like this:
```javascript
"module-deployment-order":[
    "nsg",
    "workload-net",
    "la",
    "workload-kv",
    "cloudbreak"
]
```

## Validating and Testing

The VDC automation toolkit provides two methods to help you test your updates or extensions to the provided sample deployments:

- [Deployment validation](11-deployment-validation.md) checks that your deployment templates and parameter files are valid for use with Resource Manager before running a deployment or confirming a previously run deployment has succeeded.

- [Integration testing](12-integration-testing.md) allows you to record the results of a successful deployment and use that recording to quickly confirm any updates you've made are valid before checking these changes into source control. 